# @package _global_

trainer:
  obs_norm: False
  reward_norm: False
  name: 'BC'
  qf_kwargs: null
  vf_kwargs: null # 'null' if no use of V function
  policy_type: 'TanhGaussianPolicy' # option:'VaePolicy'
  policy_kwargs: {hidden_sizes:[256, 256,]} # option: {max_action: 1., iwae=True}
  expl_kwargs: null
  trainer_kwargs: {
    total_training_steps: 1000000, # option: 100000
    policy_lr: 1E-3,
    # used for (Mix)Gaussian policy
    lr_decay: True,
    with_entropy_target: True,
    # used for VAE policy
#    beta: 0.5,
#    scheduler: False,
#    gamma: 0.95,
  }
#  exp_prefix: '${trainer.policy_type}_s_norm=${trainer.obs_norm}_TS=${trainer.trainer_kwargs.total_training_steps}_LR=${trainer.trainer_kwargs.policy_lr}_LD=${trainer.trainer_kwargs.lr_decay}_WET=${trainer.trainer_kwargs.with_entropy_target}'
  exp_prefix: "${trainer.policy_type}_s_norm=${trainer.obs_norm}\
  _TS=${trainer.trainer_kwargs.total_training_steps}\
  _LR=${trainer.trainer_kwargs.policy_lr}\
  _Beta=${trainer.trainer_kwargs.beta}"

