# @package _global_

trainer:
  obs_norm: False
  reward_norm: True
  name: 'IQL'
  qf_kwargs: {hidden_sizes:[256, 256,],}
  vf_kwargs: {hidden_sizes:[256, 256,],} # 'null' if no use of V function
  policy_type: 'GaussianPolicy'
  policy_kwargs: {hidden_sizes:[256, 256,], max_log_std: 2, min_log_std: -5, std_architecture: 'values'}
  expl_kwargs: null
  trainer_kwargs: {
    total_training_steps: 1E6, # include the steps of offline & online period, will be verified when init 'rlalg'
    discount: 0.99,
    policy_lr: 3E-4,
    qf_lr: 3E-4,
    reward_scale: 1.,
    soft_target_tau: 0.005,
    policy_weight_decay: 0,
    cosine_lr_decay: True,
    q_weight_decay: 0,
    beta: 3.0,
    quantile: 0.7,
    clip_score: 100
  }
  exp_prefix: 'beta=${trainer.trainer_kwargs.beta}_quantile=${trainer.trainer_kwargs.quantile}_clip_score=${trainer.trainer_kwargs.clip_score}'






